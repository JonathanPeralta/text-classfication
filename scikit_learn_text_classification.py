# -*- coding: utf-8 -*-
"""scikit-learn-text-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18T1zKxLWIdJL_4dxw_9pYCbU3iju-aM7
"""

import csv
import pandas as pd
import numpy as np

"""Spam or ham? [link al dataset](https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv)

#### Spam 
> Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  

#### Ham
> Oops, I'll let you know when my roommate's done
"""

# Download the dataset
#![ ! -f spam.csv ] && wget https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv

spam_or_ham = pd.read_csv("example2.csv", encoding='latin-1',error_bad_lines=False)[["v1", "v2"]]
print(spam_or_ham)
spam_or_ham.columns = ["label", "text"]
spam_or_ham.head()
# print(spam_or_ham.columns)
spam_or_ham["label"].value_counts()

"""## Vectorización

**Tokenización**: convertir un párrafo u oración a unidades (tokens), usualmente cada palabra es un token. 

En este caso, nuestra función `tokenize` es bastante simple (e ineficiente), pero sirve para nuestros simple propósito.

**Stopword removal**: eliminar tokens irrelevantes, palabras comunes y a veces signos de puntuación.

En nuestro caso, únicamente estamos eliminando los símbolos de puntuación con ayuda del set `punctuation`.
"""

import string
punctuation = set(string.punctuation)

def tokenize(sentence):
    tokens = []
    for token in sentence.split():
        new_token = []
        for character in token:
            if character not in punctuation:
                new_token.append(character.lower())
        if new_token:
            tokens.append("".join(new_token))
            # print(token)
       

    return tokens

# tokenize("Go until jurong point, crazy.. ")

spam_or_ham.head()["text"].apply(tokenize)

"""**Stemming/Lemmatization**: Convertir cada token a su forma base: {“biblioteca”, “bibliotecario”, ”bibliotecas”} → “bibliotec”.

En nuestro caso no estamos haciendo este paso, pero si es necesario, puedes revisar cosas como [NLTK - stemming](https://pythonspot.com/nltk-stemming/) o [Lemmatization Approaches with Examples in Python](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)

**One-Hot encoding**: después de la tokenización, poner en una tabla todos los tokens en el vocabulario y por cada ocurrencia de un token en un texto, marcar con un 1 en la fila correspondiente, por ejemplo considerando las dos frases siguientes:

 1. Call FREEPHONE 0800 542 0578 now!
 2. Did you call me just now ah?
 
Obtendríamos algo como esto:
 
|       | 0578 | 0800 | 542 | ah | call | did | freephone | just | me | now | you |
|-------|------|------|-----|----|------|-----|-----------|------|----|-----|-----|
| **1** | 1    | 1    | 1   | 0  | 1    | 0   | 1         | 0    | 0  | 1   | 0   |
| **2** | 0    | 0    | 0   | 1  | 1    | 1   | 0         | 1    | 1  | 1   | 1   |  

Aquí es donde entra **Scikit-Learn** a través de la clase `CountVectorizer` del módulo `sklearn.feature_extraction.text`.

[Slides]
"""

from sklearn.feature_extraction.text import CountVectorizer

demo_vectorizer = CountVectorizer(
    tokenizer = tokenize,
    binary=True
)

"""Explicación de los parámetros:  

 - **tokenizer = tokenize**: `CountVectorizer` tiene un tokenizador por default, al pasarle nuestra función lo estamos reemplazando con el que nosotros escribimos.  
 - **binary = True**: `CountVectorizer` por default en lugar de `1` cuenta el número de ocurrencias de cada token, al establecer `binary = True`, le estamos indicando que no importa cuantas veces ocurra una palabra, solamente la debe contar una vez
"""

examples = [
    "Call FREEPHONE 0800 542 0578 now!",
    "Did you call me just now ah?"
]
demo_vectorizer.fit(examples)
vectors = demo_vectorizer.transform(examples).toarray()

"""Usamos `fit` y `transform` de manera separada, aunque en este caso pudimos haber usado `fit_transform`.

**Nota**: usamos `toarray` para obtener un un *numpy array* ya que por default `transform` devuelve una [matriz dispersa](https://en.wikipedia.org/wiki/Sparse_matrix) que, mientras que es buena para no consumir memoria, no es tan amigable para mostrar cómo es que se ven los datos.
"""

headers = sorted(demo_vectorizer.vocabulary_.keys())
pd.DataFrame(vectors, columns=headers)

"""[Slides]"""

from sklearn.model_selection import train_test_split
train_text,test_text, train_labels, test_labels = train_test_split(spam_or_ham["text"], 
                                                                    spam_or_ham["label"],
                                                                    stratify=spam_or_ham["label"])
print(f"Training examples: {len(train_text)}, testing examples {len(test_text)}")

"""Una vez separados los datos, ahora si podemos comenzar a entrenar nuestro algoritmo, comenzando por generar un nuevo vectorizador:"""

real_vectorizer = CountVectorizer(tokenizer = tokenize, binary=True)

train_X = real_vectorizer.fit_transform(train_text)
test_X = real_vectorizer.transform(test_text)

train_X.shape

"""[Slides]"""

from sklearn.svm import LinearSVC

classifier = LinearSVC()
classifier.fit(train_X, train_labels)

from sklearn.metrics import accuracy_score

predicciones = classifier.predict(test_X)

accuracy = accuracy_score(test_labels, predicciones)

print(f"Accuracy: {accuracy:.4%}")



"""### Predicciones en nuevos datos"""

spam = "casa en san antonio de chiclayo"
ham = " Cascadas de Carabayllo"
tercero =  "Carmen y su esposo tienen 5 años de matrimonio y tres pequeños hijos. En junio del 2018 se mudaron a Posada del Sol en Ica consiguiendo que su sueño se volviera realidad. ¡Menorca los acompañó en este camino!."

examples = [
    spam,
    ham,
    tercero
]

examples_X = real_vectorizer.transform(examples)
predicciones = classifier.predict(examples_X)

for text, label in zip(examples, predicciones):
    print(f"{label:5} - {text}")